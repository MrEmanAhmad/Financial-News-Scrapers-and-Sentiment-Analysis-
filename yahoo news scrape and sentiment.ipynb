{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GuQVOiZNSEwo"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = WordNetLemmatizer()\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid =SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBMEmTlmSHoC"
   },
   "source": [
    "Function to scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t9Kn69n-SEwy"
   },
   "outputs": [],
   "source": [
    "def scroll(driver, timeout,ScrollNumber):\n",
    "    for i in range(1,ScrollNumber):\n",
    "        driver.execute_script(\"window.scrollTo(1,50000)\")\n",
    "        time.sleep(timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6HCvWyZOSEwz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_all_links(url,tmot,scrln):\n",
    "    print(\"\\n Getting all links to the articles\")\n",
    "    # Setup the driver. This one uses firefox with some options and a path to the geckodriver\n",
    "    driver = webdriver.Chrome()\n",
    "    # implicitly_wait tells the driver to wait before throwing an exception\n",
    "    driver.implicitly_wait(30)\n",
    "    # driver.get(url) opens the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # This starts the scrolling by passing the driver and a timeout\n",
    "    scroll(driver, timeout=tmot, ScrollNumber=scrln)\n",
    "    # Once scroll returns bs4 parsers the page_source\n",
    "    soup_a = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    # Them we close the driver as soup_a is storing the page source\n",
    "    driver.close()\n",
    "\n",
    "    # Empty array to store the links\n",
    "    links = []\n",
    "    article_urls = soup_a.findAll(\"a\",{\"class\":\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\"})\n",
    "\n",
    "\n",
    "    # Looping through all the a elements in the page source\n",
    "    for link in article_urls:\n",
    "        # link.get('href') gets the href/url out of the a element\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UPEG2-ghSEwz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_article(url):\n",
    "  user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "  headers={'User-Agent':user_agent,} \n",
    "  request=urllib.request.Request(url,None,headers)\n",
    "  try:\n",
    "    response = urllib.request.urlopen(request)\n",
    "  except:\n",
    "    time.sleep(3)\n",
    "  html = response.read()\n",
    "  bsObj1 = BeautifulSoup(html,'lxml')\n",
    "  try:    \n",
    "    article = bsObj1.find('div',{'class':'caas-body'})\n",
    "    a = article.get_text()\n",
    "    time = bsObj1.find('time')\n",
    "    t = time.get_text()\n",
    "    \n",
    "  except:\n",
    "    a = None\n",
    "    t = None\n",
    "  return t,a\n",
    " \n",
    "    \n",
    "def get_all_articles(list_urls):\n",
    "    print(\"\\n Getting all articles from links\")\n",
    "    print(\" *This will take some time please be patient*\")\n",
    "    articles = []\n",
    "    time = []\n",
    "    base = 'https://finance.yahoo.com/'\n",
    "    for link in list_urls:\n",
    "        u = base+link\n",
    "        t,a = get_article(u)\n",
    "        time.append(t)\n",
    "        articles.append(a)\n",
    "    return time,articles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RsnNftLbSEw0"
   },
   "outputs": [],
   "source": [
    "def make_df(time,articles):\n",
    "    df = pd.DataFrame({'Datetime':time,'Article':articles})\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "     totalStopwords = set([word.replace(\"'\",'') for word in stopwords.words('english')])\n",
    "     text = text.lower()\n",
    "     text = text.replace(\"'\",'')\n",
    "     text = re.sub('[^a-zA-Z]',' ',text)\n",
    "     words = text.split()\n",
    "     words = [lemma.lemmatize(word) for word in words if (word not in totalStopwords) and (len(word)>1)] # Remove stop words\n",
    "     text = \" \".join(words)\n",
    "\n",
    "     return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_sentiment_scoring(df):\n",
    "    print(\"\\n Cleaning Text\")\n",
    "    df['Article'] = df['Article'].apply(lambda x:clean_text(x))\n",
    "    df['Datetime'] = pd.to_datetime(df.Datetime)\n",
    "    print(\"\\n Analyzing Sentiment\")\n",
    "    \n",
    "    new_words = {\n",
    "        'fall':-2.0,\n",
    "        'edge':1,\n",
    "        'rise':2.0,\n",
    "        'slip':-2.0,\n",
    "        'drop':-2.0,\n",
    "        'gain':2.0,\n",
    "        'jump':2.0,\n",
    "        'climb':2.0,\n",
    "        'rally':2.0,\n",
    "        'hit':-1,\n",
    "        'end':0.4\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    sid.lexicon.update(new_words)\n",
    "    \n",
    "    desc_blob = [TextBlob(desc) for desc in df['Article']]\n",
    "    #add the sentiment metrics to the dataframe\n",
    "    df['Polarity'] = [b.sentiment.polarity for b in desc_blob]\n",
    "    df['Subjectivity'] = [b.sentiment.subjectivity for b in desc_blob]\n",
    "    #load VADER\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    #Add VADER metrics to dataframe\n",
    "    df['compound'] = [sid.polarity_scores(v)['compound'] for v in df['Article']]\n",
    "    df['neg'] = [sid.polarity_scores(v)['neg'] for v in df['Article']]\n",
    "    df['neu'] = [sid.polarity_scores(v)['neu'] for v in df['Article']]\n",
    "    df['pos'] = [sid.polarity_scores(v)['pos'] for v in df['Article']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ySGeU4rrSEw1"
   },
   "outputs": [],
   "source": [
    "def show_sentiment_by_hr(df):\n",
    "    ndf = df.set_index('Datetime').resample('H')['compound'].mean().dropna().plot(color='r', label='Sentiment')\n",
    "    ndf.legend(loc=\"upper right\")\n",
    "    ndf.set_xlabel('Datetime')\n",
    "    ndf.set_ylabel('Sentiment')\n",
    "    ndf.yaxis.label.set_color('blue')\n",
    "    ndf.xaxis.label.set_color('blue')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def show_sentiment_by_day(df):    \n",
    "    ndf = df.set_index('Datetime').resample('D')['compound'].mean().plot(color='r', label='Sentiment')\n",
    "    ndf.legend(loc=\"upper right\")\n",
    "    ndf.set_xlabel('Datetime')\n",
    "    ndf.set_ylabel('Sentiment')\n",
    "    ndf.yaxis.label.set_color('blue')\n",
    "    ndf.xaxis.label.set_color('blue')\n",
    "\n",
    "\n",
    "    \n",
    "def show_sentiment_by_week(df):    \n",
    "    ndf = df.set_index('Datetime').resample('W')['compound'].mean().plot(color='r', label='Sentiment')\n",
    "    ndf.legend(loc=\"upper right\")\n",
    "    ndf.set_xlabel('Datetime')\n",
    "    ndf.set_ylabel('Sentiment')\n",
    "    ndf.yaxis.label.set_color('blue')\n",
    "    ndf.xaxis.label.set_color('blue')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def save_to_excel(df,name):\n",
    "    df.to_excel('yahoonews/'+name+'NewsSentiment.xlsx')\n",
    "    print('File saved as: ', name+'NewsSentiment.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpOTReF0SPUl"
   },
   "source": [
    "Function to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RMMbHAuLSEw2"
   },
   "outputs": [],
   "source": [
    "def search():\n",
    "    base = 'https://finance.yahoo.com/quote/'\n",
    "    link = input('Please enter tag for example: Apple:AAPL :')\n",
    "    timeout = int(input('Please enter time to retry i.e. 3 for 3 sec. :'))\n",
    "    scroll = int(input('Please enter pages to scroll i.e. 10 for 10 scrolls :'))\n",
    "    all_links_to_visit = get_all_links(base+link,timeout,scroll)\n",
    "    ti,ar = get_all_articles(all_links_to_visit)\n",
    "    df = make_df(ti,ar)\n",
    "    df = cleaning_sentiment_scoring(df)\n",
    "    save_to_excel(df,link)\n",
    "    print('\\n **Done**')\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kUdhGPEDSEw3",
    "outputId": "70d1c8c0-8ec7-4c5d-b3e9-5c8e82e795f9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter tag for example: Apple:AAPL :aapl\n",
      "Please enter time to retry i.e. 3 for 3 sec. :1\n",
      "Please enter pages to scroll i.e. 10 for 10 scrolls :10\n",
      "\n",
      " Getting all links to the articles\n",
      "\n",
      " Getting all articles from links\n",
      " *This will take some time please be patient*\n",
      "\n",
      " Cleaning Text\n",
      "\n",
      " Analyzing Sentiment\n",
      "File saved as:  aaplNewsSentiment.xlsx\n",
      "\n",
      " **Done**\n"
     ]
    }
   ],
   "source": [
    "df = search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2BArVR6STUY"
   },
   "source": [
    "Sentiment by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1Jssnm1SEw5",
    "outputId": "c5504668-79fd-4ee5-e434-290a07fcf1b0"
   },
   "outputs": [],
   "source": [
    "#Horly sentiment\n",
    "show_sentiment_by_hr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rG8EmZ4ISWko"
   },
   "source": [
    "Sentiment by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3lOWBHiSEw6",
    "outputId": "b81b2144-af85-4a89-ba2d-db639deb9bb8"
   },
   "outputs": [],
   "source": [
    "#Daily sentiment\n",
    "show_sentiment_by_day(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scrape and analyze yahoo stock news.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
